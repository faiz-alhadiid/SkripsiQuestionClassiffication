{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bagaimana perbudakan berkembang dan kemudian meninggalkan Rusia?',\n",
       " 'Film apa yang menampilkan karakter Popeye Doyle?',\n",
       " 'Apa kepanjangan dari .com?',\n",
       " 'Profesi apa yang tertua?',\n",
       " 'Siapa yang membunuh Gandhi?']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_dataset = [\n",
    "    \"Bagaimana perbudakan berkembang dan kemudian meninggalkan Rusia?\",\n",
    "    'Film apa yang menampilkan karakter Popeye Doyle?',\n",
    "    'Apa kepanjangan dari .com?',\n",
    "    'Profesi apa yang tertua?',\n",
    "    'Siapa yang membunuh Gandhi?'\n",
    "]\n",
    "\n",
    "\n",
    "sample_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ekstraksi Fitur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitur Unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted:  ['bagaimana', 'perbudakan', 'berkembang', 'dan', 'kemudian', 'meninggalkan', 'rusia', '?']\n",
      "Remove punctuation: ['bagaimana', 'perbudakan', 'berkembang', 'dan', 'kemudian', 'meninggalkan', 'rusia']\n",
      "Splitted:  ['film', 'apa', 'yang', 'menampilkan', 'karakter', 'popeye', 'doyle', '?']\n",
      "Remove punctuation: ['film', 'apa', 'yang', 'menampilkan', 'karakter', 'popeye', 'doyle']\n",
      "Splitted:  ['apa', 'kepanjangan', 'dari', '.com', '?']\n",
      "Remove punctuation: ['apa', 'kepanjangan', 'dari', '.com']\n",
      "Splitted:  ['profesi', 'apa', 'yang', 'tertua', '?']\n",
      "Remove punctuation: ['profesi', 'apa', 'yang', 'tertua']\n",
      "Splitted:  ['siapa', 'yang', 'membunuh', 'gandhi', '?']\n",
      "Remove punctuation: ['siapa', 'yang', 'membunuh', 'gandhi']\n",
      "\n",
      "Tokens : ['.com', 'apa', 'bagaimana', 'berkembang', 'dan', 'dari', 'doyle', 'film', 'gandhi', 'karakter', 'kemudian', 'kepanjangan', 'membunuh', 'menampilkan', 'meninggalkan', 'perbudakan', 'popeye', 'profesi', 'rusia', 'siapa', 'tertua', 'yang']\n",
      "\n",
      "Vectorized:  [0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0]\n",
      "Vectorized:  [0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1]\n",
      "Vectorized:  [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Vectorized:  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1]\n",
      "Vectorized:  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1],\n",
       "  [1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "  [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1],\n",
       "  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1]],\n",
       " ['.com',\n",
       "  'apa',\n",
       "  'bagaimana',\n",
       "  'berkembang',\n",
       "  'dan',\n",
       "  'dari',\n",
       "  'doyle',\n",
       "  'film',\n",
       "  'gandhi',\n",
       "  'karakter',\n",
       "  'kemudian',\n",
       "  'kepanjangan',\n",
       "  'membunuh',\n",
       "  'menampilkan',\n",
       "  'meninggalkan',\n",
       "  'perbudakan',\n",
       "  'popeye',\n",
       "  'profesi',\n",
       "  'rusia',\n",
       "  'siapa',\n",
       "  'tertua',\n",
       "  'yang'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_unigram(sentences: List[str]):\n",
    "    tokens = set()\n",
    "    splitted_sentence = [None for i in range(len(sentences))]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        data = word_tokenize(sentence.lower())\n",
    "        print(\"Splitted: \", data)\n",
    "        data = [d for d in data if d not in string.punctuation]\n",
    "        print(\"Remove punctuation:\", data)\n",
    "        splitted_sentence[i] = data\n",
    "        tokens.update(data)\n",
    "    tokens = sorted(tokens)\n",
    "    print()\n",
    "    print(\"Tokens :\", tokens)\n",
    "    print()\n",
    "    bag_of_words = [None for i in range(len(sentences))]\n",
    "    for i, sentence in enumerate(splitted_sentence):\n",
    "        counter = Counter(sentence)\n",
    "        row = [(counter[word] if word in counter else 0) for word in tokens]\n",
    "        print(\"Vectorized: \", row)\n",
    "        bag_of_words[i] = row\n",
    "    return bag_of_words, tokens\n",
    "\n",
    "extract_unigram(sample_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitur Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[0, 0, 0, 0, 0, 1],\n",
       "  [0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 1, 0, 0, 0],\n",
       "  [0, 0, 0, 0, 1, 0]],\n",
       " ['kenapa', 'dimana', 'apa', 'berapa', 'siapa', 'bagaimana'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_question_type(sentences: List[str]):\n",
    "    label = ['kenapa', 'dimana', 'apa', 'berapa', 'siapa', 'bagaimana']\n",
    "    label_list = [None for i in range(len(sentences))]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        row = [0, 0, 0, 0, 0, 0]\n",
    "        lower: str = sentence.lower()\n",
    "        if ('kenapa' in lower or 'mengapa' in lower):\n",
    "            row[0] = 1\n",
    "        if ('dimana' in lower):\n",
    "            row[1] = 1\n",
    "        if ('berapa' in lower):\n",
    "            row[3] = 1\n",
    "        if ('siapa' in lower):\n",
    "            row[4] = 1\n",
    "        if ('bagaimana' in lower):\n",
    "            row[5] = 1\n",
    "        if (any(re.match(r'^apa', word) for word in word_tokenize(lower))):\n",
    "            row[2] = 1\n",
    "        label_list[i] = row\n",
    "    \n",
    "    return label_list, label\n",
    "\n",
    "extract_question_type(sample_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitur Word Shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_shapes(sentences: List[str]):\n",
    "    word_shapes = [\n",
    "        ('uppercase', lambda word: word.isupper() and word.isalpha()), \n",
    "        ('lowercase', lambda word: word.islower() and word.isalpha()), \n",
    "        ('mixedcase', lambda word: word.isalpha() and any(c.isupper() for c in word) and any(c.islower() for c in word)),\n",
    "        ('numeric', lambda word: re.match(r'[+-]?[0-9]+(\\\\.[0-9]+)?([Ee][+-]?[0-9]+)?', word)), \n",
    "        ('other', lambda word: not any(func(word) for _, func in word_shapes[:-1]))]\n",
    "    \n",
    "    word_shape_list = [None for i in range(len(sentences))]\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        tokenized = word_tokenize(sentence)\n",
    "        print(\"Splitted  :\", tokenized)\n",
    "        tokenized = [word for word in tokenized if word not in string.punctuation]\n",
    "        print(\"Filtered  :\", tokenized)\n",
    "        word_shape_freq = [0 for i in range(len(word_shapes))]\n",
    "        for j, word_shape in enumerate(word_shapes):\n",
    "            shape, func = word_shape\n",
    "            word_shape_freq[j] = sum(1 for word in tokenized if func(word))\n",
    "        \n",
    "        word_shape_list[i] = word_shape_freq\n",
    "    \n",
    "    return word_shape_list, [x[0] for x in word_shapes]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
